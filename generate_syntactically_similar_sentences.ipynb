{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "# import en_core_web_trf\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating syntactically similar models\n",
    "\n",
    "After trying a number of things, this approach has been the most successful which is as follows. For each word in a target sentnece, a LLM(BERT for its bidirectional properties) predicts a replacement for that word given the rest of the sentence as context. Its predictions are filtered to match the tags of the original word(POS,dep,...). Then from the X most likely predicitons that match the 'form' of the original word, a random one is chosen to replace the original. \n",
    "This process of replacing every word in the sentnece is repeated for a number of cycles. Alternative sentences are collected continuously.\n",
    "\n",
    "This process allows the sentence to maintain its original sytactic structure as each word is generated given the rest of the sentence. The incremental and random nature of the process allows the sentence to slowly trend away from the semantics of the original sentence while not introducing a large enough change that causes the prediction model to fall into confusion and fail to predict sensical replacements.\n",
    "\n",
    "Eventually, generated sentences can be filtered for dependency tree stucture of the original sentence and semantic dissimilarity from the original sentence. \n",
    "\n",
    "Possible issues:\n",
    "1. speed\n",
    "2. generated sentences appear to have random semantic distribution but this might not be the case\n",
    "\n",
    "\n",
    "Question:\n",
    "How much flexibility can there be in change words like 'are' to 'is'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nlp model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Smaller model suitable for CP\n",
    "\n",
    "# Bert model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatives = np.empty((len(sentence_split), 100), dtype=object)\n",
    "\n",
    "def generate_syntactic_similar_alternatives(sentences,num_alternatives_per_sent):\n",
    "    #replace first, then second, then third word iteratively(once)\n",
    "    all_alternatives = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        iteration = 0\n",
    "\n",
    "        for sent_idx,sentence in enumerate(sentences):\n",
    "            print(f'Finding alternatives for sentence: {\" \".join(sentence)}')\n",
    "            sentence_split = sentence.copy()\n",
    "\n",
    "\n",
    "\n",
    "            #include original sentence in list of alternatives\n",
    "            sentence_alternatives = [sentence_split]\n",
    "            \n",
    "            cycle = 0\n",
    "            # every word in the sentence will be replaced once per cycle\n",
    "            while len(sentence_alternatives) < num_alternatives_per_sent + 1:\n",
    "                # print(len(sentence_alternatives))\n",
    "\n",
    "                # replace words in random order\n",
    "                random_order = np.random.permutation(len(sentence_split))\n",
    "\n",
    "                for word_idx in random_order:\n",
    "                    original_word = nlp(' '.join(sentence_split))[word_idx]\n",
    "\n",
    "                    masked_sentence = sentence_split.copy()\n",
    "                    masked_sentence[word_idx] = tokenizer.mask_token\n",
    "                    masked_sentence = ' '.join(masked_sentence)\n",
    "\n",
    "                    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "                    mask_position = inputs.input_ids[0].tolist().index(tokenizer.mask_token_id)\n",
    "\n",
    "                    outputs = model(**inputs)\n",
    "                    predictions = outputs.logits\n",
    "\n",
    "\n",
    "                    possible_predicted = []\n",
    "                    top_k_to_consider = 100\n",
    "\n",
    "                    predicted_index = torch.topk(predictions[0, mask_position], k=top_k_to_consider).indices.tolist()\n",
    "\n",
    "                    # get top predictions, it seems only the top few make sense in many cases\n",
    "                    predicted_tokens = [tokenizer.decode([idx]) for idx in predicted_index]\n",
    "\n",
    "                    # filter by pos, dep, etc\n",
    "                    for token in predicted_tokens:\n",
    "                        temp = sentence_split.copy()\n",
    "                        temp[word_idx] = token\n",
    "                        word = nlp(' '.join(temp))[word_idx]\n",
    "                        # if not (word.pos_ == original_word.pos_ and word.dep_ == original_word.dep_ and word.text.lower() != original_word.text.lower() and word.is_stop == original_word.is_stop):\n",
    "                        #     continue\n",
    "                        if not (word.pos_ == original_word.pos_ and word.tag_ == original_word.tag_ and word.dep_ == original_word.dep_ and word.text.lower() != original_word.text.lower() and word.is_stop == original_word.is_stop):\n",
    "                            continue\n",
    "                        # check for similar subtree\n",
    "                        # if not ([j.i for j in list(word.subtree)] == [j.i for j in list(original_word.subtree)]):\n",
    "                        #     continue\n",
    "\n",
    "                        if word.text in sentence_split:\n",
    "                            continue\n",
    "\n",
    "                        possible_predicted.append(token)\n",
    "\n",
    "                    # If there are no other possible words, just keep the original word, in this case it is likely it does not have a strong semantic contribution\n",
    "                    if len(possible_predicted) == 0:\n",
    "                        possible_predicted.append(original_word.text)\n",
    "                    \n",
    "                    # choose a random word from the filtered top k preds\n",
    "                    sentence_split[word_idx] = np.random.choice([token for token in possible_predicted[:min(5, len(possible_predicted))]])\n",
    "\n",
    "                save_every = 3 # save alternative sentence every 3 cycles\n",
    "                if cycle%save_every == 0:\n",
    "                         \n",
    "                    # ensure subtrees are the same\n",
    "                    original_subtrees = [[j.i for j in list(word.subtree)] for word in nlp(\" \".join(sentence))]\n",
    "                    alternative_subtrees = [[j.i for j in list(word.subtree)] for word in nlp(\" \".join(sentence_split))]\n",
    "\n",
    "                    if(original_subtrees == alternative_subtrees):\n",
    "                        #subtrees are same so add to alternatives\n",
    "                        sentence_alternatives.append(sentence_split.copy())\n",
    "                        print(f'SAVING: {\" \".join(sentence_split)}')\n",
    "                    else:\n",
    "                        sentence_alternatives.append(sentence_split.copy())\n",
    "                        print(f'SAVING: {\" \".join(sentence_split)}') \n",
    "                        print(\"Failed subtree match\")\n",
    "                # print(sentence_split)\n",
    "                cycle += 1\n",
    "\n",
    "           \n",
    "            \n",
    "            all_alternatives.append(sentence_alternatives)\n",
    "\n",
    "    return all_alternatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"dogs are the best pets because they are loyal\",\n",
    "]\n",
    "sentences = [sentence.split(' ') for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding alternatives for sentence: dogs are the best pets because they are loyal\n",
      "SAVING: rabbits am a best wishes as i are safe\n",
      "Failed subtree match\n",
      "SAVING: giants are those strongest spirits that you keep safe\n",
      "Failed subtree match\n",
      "SAVING: miracles are a deepest things whatever they say eternal\n",
      "Failed subtree match\n"
     ]
    }
   ],
   "source": [
    "all_alternatives = generate_syntactic_similar_alternatives(sentences, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal: solutions are these hardest problems because others are different\n",
      "Alternative: chances are these smallest moments while i am dead\n",
      "Alternative: things are no worst losers until he are satisfied\n",
      "Alternative: solutions are these hardest problems because others are different\n"
     ]
    }
   ],
   "source": [
    "for target_idx,alternatives in enumerate(all_alternatives):\n",
    "    for idx,alternative in enumerate(alternatives):\n",
    "        if idx ==0:\n",
    "            print(f'Orignal: {\" \".join(alternative)}')\n",
    "        else:\n",
    "            print(f'Alternative: {\" \".join(alternative)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are usually chairs around a table for people to sit.\n"
     ]
    }
   ],
   "source": [
    "pereira_sentences = open(\n",
    "    '/Users/thomasmcgall/Desktop/research/research_push/ThomasCodeforKao/sentences_ordered.txt', 'r').read()\n",
    "pereira_sentences = np.random.choice(pereira_sentences.split('\\n'), 1)\n",
    "pereira_sentences = [sentence.split(' ') for sentence in pereira_sentences]\n",
    "\n",
    "for sent in pereira_sentences:\n",
    "    print(\" \".join(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding alternatives for sentence: There are usually chairs around a table for people to sit.\n",
      "SAVING: There become usually seats across every room that people to study\n",
      "Failed subtree match\n",
      "SAVING: There see increasingly trends towards an option which individuals to consider\n",
      "Failed subtree match\n",
      "SAVING: There seem increasingly regulations towards the item which consumers to avoid\n",
      "Failed subtree match\n"
     ]
    }
   ],
   "source": [
    "all_alternatives = generate_syntactic_similar_alternatives(pereira_sentences, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal: There seem increasingly regulations towards the item which consumers to avoid\n",
      "Alternative: There become usually seats across every room that people to study\n",
      "Alternative: There see increasingly trends towards an option which individuals to consider\n",
      "Alternative: There seem increasingly regulations towards the item which consumers to avoid\n"
     ]
    }
   ],
   "source": [
    "for target_idx,alternatives in enumerate(all_alternatives):\n",
    "    for idx,alternative in enumerate(alternatives):\n",
    "        if idx ==0:\n",
    "            print(f'Orignal: {\" \".join(alternative)}')\n",
    "        else:\n",
    "            print(f'Alternative: {\" \".join(alternative)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity, cross entropy loss of the sentence, lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "first mask all the content words in the sentence\n",
    "then generate predictions for each masked word, and from all of the predicitons select the most common one \n",
    "iteratively fill these in with some randomness\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hugging face .generate ,(can take restrictions) and edit this to \n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.40.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n",
    "change so instead of word token match constraintes it is syntax tag match\n",
    "\n",
    "also right now it is so that each word in the constraint must appear in order at some point in the string\n",
    "each time a word in the sequence is seen, it 'advances' and is eventually completed\n",
    "\n",
    "Instead match is generated on syntax tags, and the constraints are the tags of the complete sentence. In this case the model must update on every single word generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
