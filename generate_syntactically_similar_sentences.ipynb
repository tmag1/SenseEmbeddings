{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "# import en_core_web_trf\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating syntactically similar models\n",
    "\n",
    "After trying a number of things, this approach has been the most successful which is as follows. For each word in a target sentnece, a LLM(BERT for its bidirectional properties) predicts a replacement for that word given the rest of the sentence as context. Its predictions are filtered to match the tags of the original word(POS,dep,...). Then from the X most likely predicitons that match the 'form' of the original word, a random one is chosen to replace the original. \n",
    "This process of replacing every word in the sentnece is repeated for a number of cycles. Alternative sentences are collected continuously.\n",
    "\n",
    "This process allows the sentence to maintain its original sytactic structure as each word is generated given the rest of the sentence. The incremental and random nature of the process allows the sentence to slowly trend away from the semantics of the original sentence while not introducing a large enough change that causes the prediction model to fall into confusion and fail to predict sensical replacements.\n",
    "\n",
    "Eventually, generated sentences can be filtered for dependency tree stucture of the original sentence and semantic dissimilarity from the original sentence. \n",
    "\n",
    "Possible issues:\n",
    "1. speed\n",
    "2. generated sentences appear to have random semantic distribution but this might not be the case\n",
    "\n",
    "\n",
    "Question:\n",
    "How much flexibility can there be in change words like 'are' to 'is'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nlp model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Smaller model suitable for CP\n",
    "\n",
    "# Bert model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatives = np.empty((len(sentence_split), 100), dtype=object)\n",
    "\n",
    "def generate_syntactic_similar_alternatives(sentences,num_alternatives_per_sent):\n",
    "    #replace first, then second, then third word iteratively(once)\n",
    "    all_alternatives = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        iteration = 0\n",
    "\n",
    "        for sent_idx,sentence in enumerate(sentences):\n",
    "            print(f'Finding alternatives for sentence: {\" \".join(sentence)}')\n",
    "            sentence_split = sentence.copy()\n",
    "\n",
    "\n",
    "\n",
    "            #include original sentence in list of alternatives\n",
    "            sentence_alternatives = [sentence_split]\n",
    "            \n",
    "            cycle = 0\n",
    "            # every word in the sentence will be replaced once per cycle\n",
    "            while len(sentence_alternatives) < num_alternatives_per_sent + 1:\n",
    "                # print(len(sentence_alternatives))\n",
    "\n",
    "                # replace words in random order\n",
    "                random_order = np.random.permutation(len(sentence_split))\n",
    "\n",
    "                for word_idx in random_order:\n",
    "                    original_word = nlp(' '.join(sentence_split))[word_idx]\n",
    "\n",
    "                    masked_sentence = sentence_split.copy()\n",
    "                    masked_sentence[word_idx] = tokenizer.mask_token\n",
    "                    masked_sentence = ' '.join(masked_sentence)\n",
    "\n",
    "                    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "                    mask_position = inputs.input_ids[0].tolist().index(tokenizer.mask_token_id)\n",
    "\n",
    "                    outputs = model(**inputs)\n",
    "                    predictions = outputs.logits\n",
    "\n",
    "\n",
    "                    possible_predicted = []\n",
    "                    top_k_to_consider = 100\n",
    "\n",
    "                    predicted_index = torch.topk(predictions[0, mask_position], k=top_k_to_consider).indices.tolist()\n",
    "\n",
    "                    # get top predictions, it seems only the top few make sense in many cases\n",
    "                    predicted_tokens = [tokenizer.decode([idx]) for idx in predicted_index]\n",
    "\n",
    "                    # filter by pos, dep, etc\n",
    "                    for token in predicted_tokens:\n",
    "                        temp = sentence_split.copy()\n",
    "                        temp[word_idx] = token\n",
    "                        word = nlp(' '.join(temp))[word_idx]\n",
    "                        # if not (word.pos_ == original_word.pos_ and word.dep_ == original_word.dep_ and word.text.lower() != original_word.text.lower() and word.is_stop == original_word.is_stop):\n",
    "                        #     continue\n",
    "                        if not (word.pos_ == original_word.pos_ and word.tag_ == original_word.tag_ and word.dep_ == original_word.dep_ and word.text.lower() != original_word.text.lower() and word.is_stop == original_word.is_stop):\n",
    "                            continue\n",
    "                        # if not ([j.i for j in list(word.subtree)] == [j.i for j in list(original_word.subtree)]):\n",
    "                        #     continue\n",
    "\n",
    "                        if word.text in sentence_split:\n",
    "                            continue\n",
    "\n",
    "                        possible_predicted.append(token)\n",
    "\n",
    "                    # If there are no other possible words, just keep the original word, in this case it is likely it does not have a strong semantic contribution\n",
    "                    if len(possible_predicted) == 0:\n",
    "                        possible_predicted.append(original_word.text)\n",
    "                    \n",
    "                    # choose a random word from the filtered top k preds\n",
    "                    sentence_split[word_idx] = np.random.choice([token for token in possible_predicted[:min(5, len(possible_predicted))]])\n",
    "\n",
    "                save_every = 3 # save alternative sentence every 3 cycles\n",
    "                if cycle%save_every == 0:\n",
    "                         \n",
    "                    # ensure subtrees are the same\n",
    "                    original_subtrees = [[j.i for j in list(word.subtree)] for word in nlp(\" \".join(sentence))]\n",
    "                    alternative_subtrees = [[j.i for j in list(word.subtree)] for word in nlp(\" \".join(sentence_split))]\n",
    "\n",
    "                    if(original_subtrees == alternative_subtrees):\n",
    "                        #subtrees are same so add to alternatives\n",
    "                        sentence_alternatives.append(sentence_split.copy())\n",
    "                        print(f'SAVING: {\" \".join(sentence_split)}')\n",
    "                    else:\n",
    "                        sentence_alternatives.append(sentence_split.copy())\n",
    "                        print(f'SAVING: {\" \".join(sentence_split)}') \n",
    "                        print(\"Failed subtree match\")\n",
    "                # print(sentence_split)\n",
    "                cycle += 1\n",
    "\n",
    "           \n",
    "            \n",
    "            all_alternatives.append(sentence_alternatives)\n",
    "\n",
    "    return all_alternatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"dogs are the best pets because they are loyal\",\n",
    "]\n",
    "sentences = [sentence.split(' ') for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding alternatives for sentence: dogs are the best pets because they are loyal\n",
      "SAVING: pets are a strongest animals though you am intelligent\n",
      "SAVING: demons are a deepest demons though i are hungry\n",
      "SAVING: people are any worst ones within them are bad\n",
      "Failed subtree match\n"
     ]
    }
   ],
   "source": [
    "all_alternatives = generate_syntactic_similar_alternatives(sentences, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal: solutions are these hardest problems because others are different\n",
      "Alternative: chances are these smallest moments while i am dead\n",
      "Alternative: things are no worst losers until he are satisfied\n",
      "Alternative: solutions are these hardest problems because others are different\n"
     ]
    }
   ],
   "source": [
    "for target_idx,alternatives in enumerate(all_alternatives):\n",
    "    for idx,alternative in enumerate(alternatives):\n",
    "        if idx ==0:\n",
    "            print(f'Orignal: {\" \".join(alternative)}')\n",
    "        else:\n",
    "            print(f'Alternative: {\" \".join(alternative)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pereira_sentences = open(\n",
    "    '/Users/thomasmcgall/Desktop/research/research_push/ThomasCodeforKao/sentences_ordered.txt', 'r').read()\n",
    "pereira_sentences = np.random.choice(pereira_sentences.split('\\n'), 1)\n",
    "pereira_sentences = [sentence.split(' ') for sentence in pereira_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A horse is a large hoofed mammal with four long, muscular legs.\n"
     ]
    }
   ],
   "source": [
    "for sent in pereira_sentences:\n",
    "    print(\" \".join(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding alternatives for sentence: A horse is a large hoofed mammal with four long, muscular legs.\n",
      "SAVING: the dog is this short muscled man by five muscular , muscular\n",
      "Failed subtree match\n",
      "SAVING: the feature is a distinctive stylized sculpture within four dimensional , transparent\n",
      "Failed subtree match\n",
      "SAVING: the sculpture is another cylindrical curved shape with six horizontal , cylindrical\n",
      "Failed subtree match\n"
     ]
    }
   ],
   "source": [
    "all_alternatives = generate_syntactic_similar_alternatives(pereira_sentences, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal: the sculpture is another cylindrical curved shape with six horizontal , cylindrical\n",
      "Alternative: the dog is this short muscled man by five muscular , muscular\n",
      "Alternative: the feature is a distinctive stylized sculpture within four dimensional , transparent\n",
      "Alternative: the sculpture is another cylindrical curved shape with six horizontal , cylindrical\n"
     ]
    }
   ],
   "source": [
    "for target_idx,alternatives in enumerate(all_alternatives):\n",
    "    for idx,alternative in enumerate(alternatives):\n",
    "        if idx ==0:\n",
    "            print(f'Orignal: {\" \".join(alternative)}')\n",
    "        else:\n",
    "            print(f'Alternative: {\" \".join(alternative)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
