{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook\n",
    "uses a modified version of the hugging face transformers package to generate syntactically similar sentences. The approach redefines the phrasal contraint class in (generation_beam_constraint.py) and the parts of the constrained generation(generation_beam_search.py) in order to force this behavior.\n",
    "\n",
    "The constrained beam search does not work for the current version of the transformers package. The requirements for the following include transformers==4.20.1 which requires python<=3.8.\n",
    "\n",
    "Todo:\n",
    "1. Understand and change behavior of the text generation so that the constraint objects are not duplicated across beams. This is necessary in the standard use case but unecessary since all beams will have the same constraint object state at each depth of the beam. Removing this would make the whole generation much more efficient allowing for larger banks and more beams with the same time efficiency.\n",
    "2. Optimize hyperparamters including bank size for generation that is consistently high quality.\n",
    "3. Perform post generation analysis for verifying that the semantics of the generated sentences are random\n",
    "\n",
    "Understanding Constrainded Beam Search:\n",
    "https://huggingface.co/blog/constrained-beam-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The jungle tiger cub\n",
      "The jungle cat cub\n",
      "The jungle tiger tree\n",
      "The jungle tiger bar\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_input_str = \"The jungle\"\n",
    "\n",
    "'''\n",
    "APPROACH:\n",
    "\n",
    "Notes: cannot add anything large to the PhrasalContraint class as they get copied recursively\n",
    "\n",
    "1. Add a word_bank to the Phrasal contraint that is a list of list, each list corresponds to one of the target words and offers a list of alternatives \n",
    "that can replace it in the sentence.\n",
    "2. Alter the generate method to assign infinite negative score to sentence candidates that dont make progress\n",
    "\n",
    "\n",
    "For each candidate, dont any predicted next words, just expand every candidate with all possible next words\n",
    "\n",
    "'''\n",
    "constraints = [\n",
    "    PhrasalConstraint(\n",
    "        token_ids =[\n",
    "        # tokenizer(\" dog\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\"cat\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\"house\", add_special_tokens=False).input_ids[0],\n",
    "        ],\n",
    "        alt_tokens=[\n",
    "        [\n",
    "        # tokenizer(\" puma\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" cougar\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" leopard\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" cat\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" cheetah\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" tiger\", add_special_tokens=False).input_ids[0],\n",
    "\n",
    "       ],\n",
    "        [\n",
    "        tokenizer(\" ship\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" pain\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" car\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" tree\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" shoe\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" bar\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" dirt\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" rain\", add_special_tokens=False).input_ids[0],\n",
    "        tokenizer(\" cub\", add_special_tokens=False).input_ids[0],\n",
    "\n",
    "       ]\n",
    "       ]\n",
    "    \n",
    "    )\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams= 5,\n",
    "    num_return_sequences=4,\n",
    "    no_repeat_ngram_size=1, # dont repeat any words\n",
    "    remove_invalid_values=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=2\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i in range(len(outputs)):\n",
    "    print(tokenizer.decode(outputs[i], skip_special_tokens=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "import nltk  \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Load the Spacy model (Small model for CPU usage)\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Smaller model suitable for CPU\n",
    "\n",
    "# Load the corpus dataset\n",
    "dataset = load_dataset(\"generics_kb\", \"generics_kb_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sentences_to_use = np.random.choice(range(len(dataset['train'])), 100000, replace=False)\n",
    "sentence_pipe = nlp.pipe(dataset['train'][sentences_to_use[0:15000]]['generic_sentence']) #only used 3000 to avoid OOM error\n",
    "\n",
    "# Initialize the dictionary to store words by their attributes\n",
    "word_bank = {}\n",
    "\n",
    "# populate the dictionary\n",
    "for doc in sentence_pipe:\n",
    "    for w in doc:\n",
    "        # create a key tuple of word attributes\n",
    "        key = (w.pos_, w.tag_, w.dep_, w.is_stop)\n",
    "        if key not in word_bank:\n",
    "            word_bank[key] = []\n",
    "\n",
    "        # check the word is single token:\n",
    "        # rn a space is appended to each word\n",
    "        tokenized_text = tokenizer.tokenize(\" \"+w.text.lower())\n",
    "        if len(tokenized_text) > 1:\n",
    "            continue\n",
    "\n",
    "        # if the word is already in the word bank, skip it\n",
    "        if w.text.lower() in word_bank[key]:\n",
    "            continue\n",
    "        \n",
    "        # if the word is non-alphabetic, skip it\n",
    "        if not w.text.isalpha():\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Append the word text to the corresponding list in the dictionary\n",
    "        word_bank[key].append(w.text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words matching the attributes ('NOUN', 'NN', 'nsubj', False): ['cancer', 'citizenship', 'computer', 'silk', 'cache', 'graphic', 'protection', 'communication', 'nutrition', 'pressure', 'love', 'fish', 'cream', 'deer', 'wind', 'sun', 'light', 'pregnancy', 'wildlife', 'blade', 'arthritis', 'ambition', 'infection', 'food', 'health', 'function', 'religion', 'material', 'agriculture', 'dehydration', 'oil', 'development', 'football', 'narration', 'grain', 'fog', 'photography', 'disturbance', 'stress', 'cooking', 'succession', 'weather', 'plant', 'piracy', 'selection', 'fraud', 'purity', 'pork', 'perception', 'drawing', 'sugar', 'illness', 'fluid', 'information', 'water', 'neon', 'sleep', 'sexism', 'war', 'confidence', 'furniture', 'poverty', 'stock', 'cocaine', 'planting', 'dust', 'air', 'faith', 'tolerance', 'use', 'pollution', 'turbulence', 'sky', 'milk', 'flight', 'energy', 'influenza', 'span', 'acid', 'aspiration', 'time', 'current', 'ice', 'throttle', 'chart', 'growth', 'smoking', 'glass', 'sustainability', 'sunlight', 'rate', 'soccer', 'nicotine', 'destruction', 'music', 'control', 'money', 'force', 'mass', 'testing', 'toxicity', 'pizza', 'insulation', 'life', 'colour', 'meat', 'timing', 'business', 'right', 'organism', 'violence', 'depression', 'lightning', 'exposure', 'stability', 'creativity', 'bombing', 'toast', 'beef', 'diversity', 'disease', 'fur', 'inheritance', 'journalism', 'elegance', 'fire', 'coffee', 'suicide', 'understanding', 'emulation', 'interaction', 'principal', 'ion', 'rape', 'planning', 'quartz', 'technology', 'migration', 'property', 'science', 'deprivation', 'obesity', 'paint', 'tissue', 'waste', 'intuition', 'writing', 'therapy', 'fertilizer', 'correspondent', 'range', 'damage', 'interpretation', 'act', 'mold', 'yeast', 'society', 'law', 'monitoring', 'plate', 'intellect', 'poultry', 'complexity', 'abstraction', 'structure', 'caffeine', 'mankind', 'animal', 'effect', 'beer', 'seed', 'surgery', 'sex', 'star', 'autumn', 'country', 'creation', 'radio', 'board', 'capitalist', 'humanity', 'rice', 'blood', 'arm', 'tent', 'source', 'vomiting', 'gasoline', 'habitat', 'tea', 'research', 'mind', 'gum', 'gardening', 'safety', 'system', 'magic', 'hunger', 'privacy', 'mathematics', 'vapor', 'darkness', 'wood', 'velocity', 'income', 'privatization', 'fertility', 'variation', 'mediation', 'earth', 't', 'figure', 'uptake', 'mixture', 'voltage', 'capture', 'referendum', 'shape', 'irritation', 'swimming', 'tobacco', 'education', 'paralysis', 'land', 'pest', 'capitalism', 'knowledge', 'skin', 'flame', 'prayer', 'bass', 'alcohol', 'fruit', 'reproduction', 'inequality', 'abortion', 'filament', 'addiction', 'work', 'divorce', 'temperature', 'sting', 'body', 'fortnight', 'government', 'observed', 'object', 'care', 'grey', 'shock', 'optimization', 'livestock', 'grease', 'motion', 'humidity', 'excellence', 'diet', 'judgement', 'cell', 'ball', 'winter', 'loss', 'paper', 'optimism', 'heat', 'size', 'church', 'compost', 'charcoal', 'state', 'sickness', 'delusion', 'basis', 'symmetry', 'cause', 'proceeding', 'marble', 'feeling', 'snow', 'vision', 'production', 'corn', 'culture', 'chemotherapy', 'mist', 'item', 'democracy', 'yoga', 'contraction', 'storage', 'debate', 'baby', 'activity', 'management', 'processor', 'psychology', 'abuse', 'inflammation', 'insight', 'healing', 'wear', 'productivity', 'theology', 'virtue', 'variability', 'smoke', 'moisture', 'fear', 'prostitution', 'action', 'philosophy', 'pride', 'humility', 'massage', 'web', 'governance', 'erosion', 'gravity', 'wound', 'person', 'crime', 'puberty', 'language', 'decay', 'cave', 'counter', 'syrup', 'cycling', 'home', 'solidarity', 'addition', 'forgiveness', 'dioxide', 'chess', 'deployment', 'mortality', 'stroke', 'death', 'temptation', 'magnification', 'digestion', 'glue', 'oak', 'exchange', 'friction', 'concentration', 'precipitation', 'pig', 'flexibility', 'silver', 'titanium', 'rocket', 'sociology', 'happiness', 'gland', 'bug', 'analysis', 'juice', 'satisfaction', 'replication', 'cinema', 'theory', 'anxiety', 'training', 'oversight', 'anthropology', 'space', 'poisoning', 'wiring', 'sculpture', 'shrimp', 'sulfur', 'sin', 'salt', 'candy', 'cocoa', 'point', 'offspring', 'invitation', 'age', 'overrun', 'deficiency', 'exercise', 'schizophrenia', 'pain', 'power', 'compassion', 'fiction', 'degradation', 'engineering', 'grass', 'intelligence', 'adaptation', 'code', 'modeling', 'shame', 'freedom', 'evidence', 'bone', 'intake', 'denial', 'trade', 'achievement', 'curve', 'treatment', 'authority', 'meal', 'racism', 'race', 'fiber', 'fatigue', 'brain', 'chain', 'planet', 'behavior', 'species', 'mail', 'weight', 'pine', 'ink', 'unemployment', 'ash', 'metabolism', 'ozone', 'oxygen', 'performance', 'c', 'encryption', 'category', 'availability', 'chocolate', 'mystery', 'vegetation', 'height', 'text', 'leadership', 'worship', 'hair', 'spring', 'laughter', 'gold', 'hatred', 'repair', 'boom', 'evolution', 'advertising', 'max', 'mapping', 'tradition', 'corruption', 'housing', 'carbon', 'history', 'salvation', 'depreciation', 'sanitation', 'drive', 'copper', 'testosterone', 'wisdom', 'suppression', 'plastic', 'invention', 'record', 'touch', 'queen', 'pepper', 'cad', 'substrate', 'foliage', 'biodiversity', 'protein', 'mountain', 'disorder', 'regulation', 'sand', 'media', 'vigilance', 'commerce', 'projection', 'illumination', 'man', 'packaging', 'integrity', 'counseling', 'shower', 'bias', 'detainee', 'number', 'identification', 'inflation', 'library', 'picture', 'medicine', 'level', 'transition', 'experimentation', 'basketball', 'radiation', 'imagery', 'marriage', 'anger', 'wave', 'hockey', 'inhabit', 'lust', 'bedroom', 'rain', 'moist', 'advocacy', 'prevention', 'roam', 'harvest', 'contemplation', 'evil', 'beam', 'neuron', 'accountability', 'meaning', 'emptiness', 'poison', 'quality', 'pesticide', 'childhood', 'fairness', 'vitality', 'woman', 'flow', 'value', 'venom', 'egg', 'volume', 'family', 'persuasion', 'audience', 'population', 'documentation', 'transportation', 'injunction', 'cod', 'maintenance', 'pornography', 'determination', 'sediment', 'prey', 'formation', 'email', 'effort', 'alum', 'circumcision', 'child', 'teamwork', 'teacher', 'dependence', 'flesh', 'frequency', 'saliva', 'programming', 'resentment', 'posture', 'assault', 'algae', 'grooming', 'logic', 'security', 'translation', 'mercury', 'living', 'length', 'baptism', 'mineral', 'childbirth', 'fox', 'coli', 'extinction', 'stain', 'incest', 'tint', 'season', 'maturity', 'soul', 'behaviour', 'support', 'tenure', 'impairment', 'conservation', 'chest', 'hour', 'atom', 'equipment', 'phosphate', 'building', 'tracking', 'condition', 'fever', 'charge', 'belief', 'bank', 'biology', 'reality', 'peace', 'golf', 'load', 'conduct', 'globalization', 'wheat', 'jumper', 'trout', 'asbestos', 'cortisol', 'cuisine', 'finance', 'clothing', 'disability', 'poetry', 'skepticism', 'ale', 'muscle', 'cessation', 'hepatitis', 'mode', 'chloride', 'rainfall', 'socialism', 'malnutrition', 'wire', 'world', 'insulin', 'integration', 'dye', 'ignorance', 'imbalance', 'conflict', 'meditation', 'cum', 'worm', 'plaster', 'machine', 'nose', 'butter', 'retention', 'morality', 'cough', 'deterioration', 'amber', 'recycling', 'entry', 'hypertension', 'ratio', 'brunch', 'fabric', 'art', 'program', 'recollection', 'wealth', 'urine', 'extraction', 'activation', 'nature', 'oxide', 'saturation', 'ultraviolet', 'kidnapping', 'absorption', 'plasma', 'mythology', 'overload', 'labor', 'liquid', 'pneumonia', 'chemistry', 'midnight', 'device', 'imitation', 'duplication', 'machinery', 'rivalry', 'formulation', 'annotation', 'uncertainty', 'imprisonment', 'manufacturing', 'rejection', 'employment', 'speed', 'soda', 'drinking', 'detection', 'approach', 'involvement', 'profitability', 'asteroid', 'soap', 'miscarriage', 'armor', 'math', 'lead', 'lighting', 'alcoholism', 'tennis', 'construction', 'strength', 'wheel', 'root', 'awareness', 'sight', 'h', 'sample', 'harassment', 'cholesterol', 'tin', 'store', 'spot', 'sage', 'reliability', 'college', 'sound', 'granite', 'wagon', 'pump', 'balance', 'coat', 'pollen', 'equality', 'plaque', 'individual', 'converter', 'change', 'surprise', 'fuel', 'composition', 'breathing', 'list', 'group', 'style', 'ecology', 'veterinarian', 'proof', 'secretion', 'string', 'matter', 'piece', 'soil', 'place', 'coverage', 'loneliness', 'passion', 'deck', 'insurance', 'voting', 'experience', 'e', 'roof', 'curb', 'factor', 'foot', 'cost', 'confusion', 'identity', 'lung', 'capacity', 'electricity', 'community', 'literacy', 'valve', 'tank', 'adjustment', 'angle', 'spending', 'morning', 'study', 'gain', 'simulation', 'nursing', 'hygiene', 'closing', 'ammonia', 'propagation', 'supremacy', 'emotion', 'significance', 'ship', 'spirituality', 'line', 'misery', 'input', 'estate', 'righteousness', 'failure', 'abstinence', 'jungle', 'advertisement', 'powder', 'car', 'fat', 'preparation', 'wax', 'washing', 'eviction', 'autism', 'supply', 'truth', 'beauty', 'step', 'wine', 'baseball', 'licence', 'investigation', 'user', 'fashion', 'fusion', 'deforestation', 'bread', 'homicides', 'contact', 'physics', 'internet', 'd', 'service', 'resistance', 'climbing', 'impulse', 'esteem', 'kernel', 'apprehension', 'custody', 'proficiency', 'shooting', 'reduction', 'intimacy', 'problem', 'loyalty', 'brass', 'rail', 'enthusiast', 'competition', 'form', 'confession', 'exhaust', 'baggage', 'organization', 'architecture', 'capital', 'dance', 'vessel', 'traffic', 'familiarity', 'teaching', 'calcium', 'slate', 'medication', 'flower', 'design', 'drift', 'variable', 'policy', 'scale', 'chap', 'city', 'polymer', 'theft', 'civilization', 'masturbation', 'ethanol', 'bandwidth', 'debt', 'injure', 'coal', 'ventilation', 'squash', 'oxidation', 'deposition', 'mustard', 'industry', 'fishing', 'chlorine', 'moth', 'expedition', 'reaction', 'vinyl', 'shovel', 'lag', 'steel', 'tuberculosis', 'orientation', 'panic', 'bob', 'animation', 'contamination', 'bypass', 'luck', 'fir', 'tension', 'lock', 'eye', 'cryptography', 'poker', 'square', 'heart', 'rush', 'climate', 'entropy', 'cloning', 'asphalt', 'acupuncture', 'charity', 'developer', 'honor', 'slope', 'price', 'differentiation', 'fermentation', 'male', 'pleasure', 'plutonium', 'empathy', 'injury', 'publishing', 'rubber', 'deduction', 'grace', 'prosperity', 'intolerance', 'odor', 'replacement', 'competitiveness', 'television', 'police', 'pasture', 'scarcity', 'axis', 'feminism', 'serum', 'admiration', 'metabolic', 'sense', 'gas', 'tourism', 'mobility', 'predictor', 'environment', 'obstruction', 'marrow', 'hazard', 'bark', 'order', 'slime', 'irony', 'marketing', 'intensity', 'rotation', 'ant', 'boy', 'substance', 'fetus', 'covering', 'polish', 'ability', 'diffusion', 'eating', 'mutation', 'review', 'unit', 'flooding', 'cube', 'potassium', 'valuation', 'wrap', 'consolidation', 'combustion', 'consent', 'agent', 'ingestion', 'contraception', 'parking', 'legitimacy', 'consumption', 'universe', 'racing', 'element', 'demand', 'mining', 'reflection', 'cognition', 'learning', 'dancing', 'semen', 'scripture', 'printing', 'defect', 'straw', 'memory', 'compression', 'trapping', 'mourn', 'thunder', 'probability', 'salmon', 'zinc', 'coconut', 'division', 'deviation', 'box', 'process', 'spinach', 'component', 'software', 'redemption', 'cone', 'surveillance', 'example', 'thing', 'density', 'honey', 'creep', 'consumer', 'content', 'lack', 'absurdity', 'notation', 'pattern', 'chance', 'surface', 'ore', 'exhaustion', 'film', 'torque', 'sac', 'citizen', 'base', 'classification', 'restoration', 'justice', 'student', 'attention', 'belly', 'magnesium', 'fulfillment', 'bullying', 'summer', 'relationship', 'typing', 'covenant', 'absence', 'hand', 'trauma', 'pit', 'blight', 'liquor', 'confidentiality', 'sunshine', 'adolescence', 'waterfall', 'potion', 'principle', 'way', 'spirit', 'room', 'output', 'speaker', 'drought', 'risk', 'corrosion', 'aircraft', 'doctor', 'convert', 'drug', 'cycle', 'banking', 'homelessness', 'technique', 'pussy', 'birth', 'pearl', 'accommodation', 'injection', 'rock', 'sen', 'area', 'embodiment', 'attempt', 'sperm', 'childcare', 'sovereignty', 'intrusion', 'video', 'barrel', 'tree', 'spread', 'breakdown', 'intercourse', 'orange', 'strategy', 'merchant', 'soup', 'monopoly', 'currency', 'farm', 'type', 'furnace', 'dependency', 'wolf', 'equilibrium', 'connector', 'painting', 'arousal', 'durability', 'clay', 'counting', 'promotion', 'proliferation', 'flour', 'gun', 'squid', 'incarceration', 'literature', 'recovery', 'reflex', 'head', 'defense', 'harmony', 'shortage', 'consciousness', 'dancer', 'nitrogen', 'steam', 'transport', 'cheese', 'definition', 'aluminum', 'arson', 'concern', 'color', 'sadness', 'grief', 'pill', 'pack', 'courage', 'clout', 'infertility', 'mosquito', 'enterprise', 'exclusive', 'triangle', 'transparency', 'night', 'recording', 'obedience', 'suspension', 'lending', 'manipulation', 'virus', 'cement', 'sport', 'nation', 'basil', 'fungus', 'expenditure', 'duration', 'election', 'eclipse', 'independence', 'discussion', 'profile', 'biomass', 'resolution', 'consideration', 'response', 'investing', 'application', 'cold', 'petroleum', 'consistency', 'dysfunction', 'dissemination', 'hindsight', 'transcription', 'transplant', 'exterior', 'progress', 'browser', 'iron', 'generation', 'blindness', 'sexuality', 'squirrel', 'notice', 'severity', 'surfing', 'observation', 'physiology', 'rabbi', 'metal', 'site', 'acceleration', 'barrier', 'deterrence', 'containment', 'therapist', 'repression', 'lip', 'nucleus', 'access', 'nationalism', 'asthma', 'concrete', 'skiing', 'playing', 'incidence', 'aggregation', 'arbitration', 'lumber', 'adoption', 'pistol', 'cord', 'correlation', 'street', 'feeding', 'manure', 'jumping', 'patience', 'bridge', 'screen', 'restriction', 'webs', 'fact', 'bladder', 'day', 'friendship', 'moon', 'jazz', 'falsehood', 'malaria', 'trading', 'journey', 'paranoia', 'grenade', 'youth', 'cherry', 'agency', 'conception', 'sweat', 'profession', 'negotiation', 'movement', 'burning', 'isolation', 'stimulation', 'iodine', 'fall', 'irrigation', 'barley', 'deception', 'brow', 'recognition', 'resonance', 'resource', 'sodium', 'pin', 'patriotism', 'pigment', 'clarity', 'garlic', 'host', 'increase', 'archetype', 'aunt', 'allergy', 'separation', 'compound', 'regression', 'participation', 'silence', 'limb', 'pacing', 'excess', 'lad', 'market', 'comprehension', 'hormone', 'plat', 'coding', 'conviction', 'sensor', 'struggle', 'profit', 'similarity', 'acne', 'cluster', 'class', 'daddy', 'vaccine', 'feature', 'reason', 'anarchism', 'grounding', 'leather', 'discharge', 'evaluation', 'song', 'inconsistency', 'agreement', 'ginger', 'collision', 'transit', 'transformation', 'nerve', 'realism', 'court', 'weekend', 'astronomy', 'scent', 'commonplace', 'blister', 'sheep', 'dissatisfaction', 'infinity', 'ground', 'specification', 'opera', 'dare', 'servant', 'criticism', 'doctrine']\n"
     ]
    }
   ],
   "source": [
    "example_key = ('NOUN', 'NN', 'nsubj', False)  # Example tuple key\n",
    "matching_words = word_bank.get(example_key, [])\n",
    "print(\"Words matching the attributes {}: {}\".format(example_key, matching_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats NOUN NNS nsubj False\n",
      "love VERB VBP ROOT False\n",
      "dogs NOUN NNS dobj False\n"
     ]
    }
   ],
   "source": [
    "sent = \"cats love dogs\"\n",
    "doc = nlp(sent)\n",
    "for w in doc:\n",
    "    print(w.text, w.pos_, w.tag_, w.dep_, w.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_tokens(sent,max_bank_size,word_bank,tokenizer,nlp):\n",
    "\n",
    "    input_words = input_tokens = re.findall(r'\\w+|[^\\w\\s]', sent)\n",
    "    \n",
    "    input_tokens = [\n",
    "        tokenizer(\" \"+word, add_special_tokens=False).input_ids[0] for word in input_words\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    alt_words = []\n",
    "    doc = nlp(sent)\n",
    "\n",
    "\n",
    "    for w in doc:\n",
    "        key = (w.pos_, w.tag_, w.dep_, w.is_stop)\n",
    "        if w.is_stop:\n",
    "            #NOTE: this may not be single token\n",
    "            matching_words = [w.text.lower()]\n",
    "\n",
    "        # if the word is a special character, just return a list with the word itself\n",
    "        elif not w.text.isalpha():\n",
    "            matching_words = [w.text.lower()]\n",
    "\n",
    "        else:\n",
    "            # if there are no matching words in the bank raise an error\n",
    "            if key not in word_bank:\n",
    "                raise ValueError(\"No matching words in the word bank for the key: {}\".format(key))\n",
    "\n",
    "            matching_words = word_bank.get(key, [])\n",
    "\n",
    "            # chose random subjection of matching words to limit the size of the banks and introduce randomness\n",
    "            matching_words = np.random.choice(matching_words, min(max_bank_size, len(matching_words)), replace=False)\n",
    "\n",
    "\n",
    "        alt_words.append(matching_words)\n",
    "\n",
    "    alt_tokens = []\n",
    "\n",
    "    for i in range(len(alt_words)):\n",
    "        alt_token_per_word = []\n",
    "        for j in range(len(alt_words[i])):\n",
    "            alt_token_per_word.append(tokenizer(\" \"+alt_words[i][j], add_special_tokens=False).input_ids[0])\n",
    "    \n",
    "\n",
    "        alt_tokens.append(alt_token_per_word)\n",
    "\n",
    "    return input_tokens, alt_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize('selection')\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " people\n",
      " want\n",
      " things\n",
      " more\n",
      " than\n",
      " anything\n"
     ]
    }
   ],
   "source": [
    "outputs.shape\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    for k in range(1,len(outputs[i])):\n",
    "        print(tokenizer.decode(outputs[i][k], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_syntactic_similars(sentences, num_alternatives, num_beams, max_bank_size, word_bank, tokenizer, nlp):\n",
    "    encoder_input_str = \" \"\n",
    "    \n",
    "    all_alt_sentences = []\n",
    "\n",
    "    for sent_idx, sent in enumerate(sentences):\n",
    "        print(f\"Generating alternatives for sentence {sent_idx+1}/{len(sentences)}\")\n",
    "        sent = sent.lower()\n",
    "\n",
    "        # generate banks of certain size randomly selected\n",
    "        input_tokens, alternative_tokens = generate_tokens(sent, max_bank_size, word_bank, tokenizer, nlp)\n",
    "\n",
    "        alt_sentences = []\n",
    "        for i in range(num_alternatives):\n",
    "\n",
    "            # Remove all previously used words from the alternative tokens\n",
    "            for alt_sentence in alt_sentences:\n",
    "                for token_idx in range(1, len(alt_sentence)):\n",
    "                    token = alt_sentence[token_idx]\n",
    "                    try:\n",
    "                        if len(alternative_tokens[token_idx-1]) > 10:\n",
    "                            alternative_tokens[token_idx-1].remove(token)\n",
    "                    except:\n",
    "                        print(f\"Error: token [{tokenizer.decode(token)}] not found in list\")\n",
    "\n",
    "            constraints = [\n",
    "                PhrasalConstraint(\n",
    "                    token_ids=input_tokens,\n",
    "                    alt_tokens=alternative_tokens\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                constraints=constraints,\n",
    "                num_beams=num_beams,\n",
    "                num_return_sequences=1,\n",
    "                remove_invalid_values=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                max_new_tokens=len(input_tokens)\n",
    "            )\n",
    "\n",
    "            alt_sentence = outputs[0]\n",
    "            alt_sentences.append(alt_sentence)\n",
    "\n",
    "        all_alt_sentences.append(alt_sentences)\n",
    "\n",
    "    return [[tokenizer.decode(alt[1:], skip_special_tokens=True) for alt in alt_sentence] for alt_sentence in all_alt_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: \n",
      "cats love dogs more when the time is right\n",
      "alternative sentences: \n",
      " companies want people more when the price is right\n",
      " patients receive drugs more when the risk is low\n",
      "input sentence: \n",
      "the jungle is a place of wonder\n",
      "alternative sentences: \n",
      " the act is a crime of violence\n",
      " the colour is a mixture of red\n",
      "input sentence: \n",
      "the cat is a predator when hungry\n",
      "alternative sentences: \n",
      " the fruit is a kind when ripe\n",
      " the season is a success when young\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"cats love dogs more when the time is right\", \"the jungle is a place of wonder\",\"the cat is a predator when hungry\" ]\n",
    "\n",
    "alt_sentences = create_syntactic_similars(sentences,num_alternatives=2, num_beams=60, max_bank_size=400,word_bank=word_bank,tokenizer=tokenizer,nlp=nlp)\n",
    "\n",
    "for idx,sent in enumerate(sentences):\n",
    "    print(\"input sentence: \")\n",
    "    print(sent)\n",
    "    print(\"alternative sentences: \")\n",
    "\n",
    "    for alt in alt_sentences[idx]:\n",
    "        print(alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "1\n",
      "1\n",
      "51\n",
      "  customers need when you want\n",
      "  tools like when you use\n"
     ]
    }
   ],
   "source": [
    "# NOTE: ERROR arena is multitoken\n",
    "sentences = [\"trees know when you live\"]\n",
    "\n",
    "alt_sentences = create_syntactic_similars(sentences, 2, word_bank,tokenizer,nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' the sea, and', ' the author, and']]\n"
     ]
    }
   ],
   "source": [
    "print(alt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'you', 'live', 'with', 'someone', 'who', 'has', 'a', 'temper', ',', 'a', 'very', 'bad', 'temper', ',', 'a', 'very', ',', 'very', 'bad', 'temper', ',', 'you', 'learn', 'to', 'play', 'around', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sent = \"when you live with someone who has a temper, a very bad temper, a very, very bad temper, you learn to play around.\"\n",
    "\n",
    "# Use regular expressions to split the sentence into words and punctuation\n",
    "input_tokens = re.findall(r'\\w+|[^\\w\\s]', sent)\n",
    "\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'you', 'live', 'with', 'someone', 'who', 'has', 'a', 'temper,', 'a', 'very', 'bad', 'temper,', 'a', 'very,', 'very', 'bad', 'temper,', 'you', 'learn', 'to', 'play', 'around.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"when you live with someone who has a temper, a very bad temper, a very, very bad temper, you learn to play around.\" \n",
    "input_tokens = [\n",
    "    word for word in sent.split()\n",
    "]\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the textgrids\n",
    "\n",
    "# Rstories are the names of the training (or Regression) stories, which we will use to fit our models\n",
    "Rstories = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy', \n",
    "            'life', 'myfirstdaywiththeyankees', 'naked', \n",
    "            'odetostepfather', 'souls', 'undertheinfluence']\n",
    "\n",
    "# Pstories are the test (or Prediction) stories (well, story), which we will use to test our models\n",
    "Pstories = ['wheretheressmoke']\n",
    "\n",
    "allstories = Rstories + Pstories\n",
    "\n",
    "# Load TextGrids\n",
    "from stimulus_utils import load_grids_for_stories\n",
    "grids = load_grids_for_stories(allstories)\n",
    "\n",
    "# Load TRfiles\n",
    "from stimulus_utils import load_generic_trfiles\n",
    "trfiles = load_generic_trfiles(allstories)\n",
    "\n",
    "# Make word and phoneme datasequences\n",
    "from dsutils import make_word_ds, make_phoneme_ds\n",
    "wordseqs = make_word_ds(grids, trfiles) # dictionary of {storyname : word DataSequence}\n",
    "phonseqs = make_phoneme_ds(grids, trfiles) # dictionary of {storyname : phoneme DataSequence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1839 words in the story called 'wheretheressmoke'\n"
     ]
    }
   ],
   "source": [
    "wheretheressmoke = wordseqs[\"wheretheressmoke\"]\n",
    "print (\"There are %d words in the story called 'wheretheressmoke'\" % len(list(wheretheressmoke.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasmcgall/opt/anaconda3/envs/envCon/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "punctuation_model= PunctuationModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = ' '.join(wheretheressmoke.data)\n",
    "punctuation_model = PunctuationModel()\n",
    "# Restore Punctuation\n",
    "punctuated_output = punctuation_model.restore_punctuation(input_text)\n",
    "# Split sentences by Punctuation\n",
    "story_sentences = nltk.sent_tokenize(punctuated_output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i reached over and secretly undid my seatbelt and when his foot hit the brake at the red light, i flung open the door and i ran.\n",
      "i had no shoes on, i was crying, i had no wallet, but i was ok because i had my cigarettes and i didn't want any part of freedom if i didn't have my cigarettes.\n",
      "when you live with someone who has a temper, a very bad temper, a very, very bad temper, you learn to play around.\n",
      "that you learn this time i'll play possum and next time i'll just be real nice, or i'll say yes to everything, or you make yourself scarce or you run.\n",
      "and this was one of the times when you just run and as i was running, i thought this was a great place to jump out, because there were big lawns and there were cul de sacs and sometimes he would come after me and drive and yell stuff at me to get back in, get back in.\n",
      "and i was like no, i'm out of here, this is great.\n",
      "and i went and hid behind a cabana and he left and i had my cigarettes and, uh, i started to walk in this beautiful neighborhood.\n",
      "it was ten thirty at night and it was silent and lovely and there was no sound, except for sprinklers: ch, ch, ch, ch {ig}, ch, ch, ch, ch, {ig}.\n",
      "and i was enjoying myself and enjoying the absence of anger, and enjoying these few hours i knew i'd have of freedom.\n",
      "and just to perfect it, i thought i'll have a smoke.\n",
      "and then it occurred to me, with horrifying speed, i don't have a light.\n",
      "just then, as if in answer, i see a figure up ahead.\n",
      "who is that?\n",
      "it's not him.\n",
      "ok, they don't have a dog.\n",
      "who is that?\n",
      "what, uh, what are they doing out on this suburban street?\n",
      "and the person comes closer and i could see it's a woman.\n",
      "and then i can see she has her hands in her face.\n",
      "oh, she's crying.\n",
      "and then she sees me and she composes herself and she gets closer and i see she has no shoes on.\n",
      "she has no shoes on and she's crying and she's out on the street.\n",
      "street, i recognize her, though i've never met her.\n",
      "and just as she passes me, she says you got a cigarette?\n",
      "and i say you got a light?\n",
      "and she says: damn, i hope so.\n",
      "and then sh?\n",
      "first she digs into her cutoffs in the front- nothing- and then digs in the back, and then she has this vest on that has fifty million little pockets on it and she's checking and checking and it's looking bad.\n",
      "it's looking very bad.\n",
      "she digs back in the front again, deep, deep, and she pulls out a pack of matches that had been laundered at least once.\n",
      "ukgh, we open it up and there is one match inside.\n",
      "ok, oh, my god, this takes on.\n",
      "it's like nasa now.\n",
      "we got to like: oh, how are we gonna do it?\n",
      "ok, and we, we hunker down, we crouch on the ground and where's the wind coming from?\n",
      "we're stopping.\n",
      "i take out my cigarettes.\n",
      "let's get the cigarettes ready.\n",
      "oh, my brand, she says, not surprising.\n",
      "and we both have our cigarettes at the ready.\n",
      "she strikes once, nothing.\n",
      "she strikes again: yes, fire, puff, inhale, mm.\n",
      "sweet kiss of that cigarette.\n",
      "and we sit there and we're loving the nicotine and we both need this.\n",
      "right now i can tell the night's been tough.\n",
      "immediately we start to reminisce about our thirty second relationship.\n",
      "i didn't think that was gonna happen, me neither.\n",
      "oh, man, that was close.\n",
      "oh, i'm so lucky i saw you, yeah.\n",
      "then she surprises me by saying: what was the fight about?\n",
      "and i say: wha, what are they all about?\n",
      "and she said: i know what you mean.\n",
      "she said: was it a bad one.\n",
      "and and i said, you know, like medium, she said, oh, and we start to trade stories about our lives.\n",
      "we're both from up north, we're both kind of newish to the neighborhood.\n",
      "this is in florida.\n",
      "we both went to college- not great colleges, but, man, we graduated and i'm actually finding myself a little jealous of her because she has this really cool job: washing dogs.\n",
      "she had horses back home and she really loves animals and she wants to be a vet and i'm like man, you're halfway there, i'm a waitress at an ice cream parlor, so, um, that's not.\n",
      "i don't know where i want to be, but i know it's not that.\n",
      "and then it gets a little deeper {cg} and we share some other stuff about what our lives are, like things that i can't ever tell people at home.\n",
      "this girl, i can tell her the really ugly stuff and she still understands how it can still be pretty.\n",
      "she understands, like, how nice he's gonna be when i get home and how sweet that'll be.\n",
      "we are chain smoking off each other- oh, that's almost out, come on.\n",
      "and we, we go through this entire pack until it's gone and then i say: you know what?\n",
      "uh, this is a little funny, but you're gonna have to show me the way to get home because, although i'm twenty three years old, i don't have my driver's license yet.\n",
      "and i just jumped out right when i needed to.\n",
      "and she says, well, why don't you come back to my house and i'll give you a ride?\n",
      "i say, ok, great, and we start walking and, uh, we get to this, um, lots of uh lights and uh, the roads are getting wider and wider and there's more cars and i see, um, lots of stores, you know, laundromats and dollar stores and emergecenters.\n",
      "and then we cross over us one and uh, she leads me to some place and i think no, but yes, carl's efficiency apartments.\n",
      "this girl lives there and it's horrible and it's lit up so bright just to illuminate the horribleness of it.\n",
      "it's the kind of place where you drive your car right up and the door's right there and there's fifty million cigarette butts outside and there's like doors one through seven, and you know, behind every single door there's some horrible misery going on.\n",
      "there's someone crying or drunk, or lonely or cruel, and i think, oh god, she lives here.\n",
      "how awful.\n",
      "we go to the door, door number four, and she very, very quietly keys in as soon as the door opens, i hear the blare of television come out and on the blue light of the television, the smoke of a hundred cigarettes in that little crack of light and i hear the man and he says where were you?\n",
      "and she says, never mind, i'm back.\n",
      "and he says you alright?\n",
      "and she says yeah, i'm alright.\n",
      "and then she turns to me and says you want a beer.\n",
      "and he says who the fuck is that?\n",
      "and she pulls me over and he sees me and he says, oh hey, i'm not a threat.\n",
      "just then he takes a drag of his cigarette, a very hard, hard drag, you know the kind that makes the end of it really heat up, hot, hot, hot and long.\n",
      "and it's a little scary and i follow the cigarette down because i'm afraid of that head falling off.\n",
      "and i'm surprised when i see in the crook of his arm a little boy sleeping, a toddler and i think.\n",
      "and just then the girl reaches underneath the bed and takes out a carton and she taps out the last s pack of cigarettes in there and on the way up she kisses the little boy and then she kisses the man and the man says again, you alright?\n",
      "and she says yeah, i'm just gonna go out and smoke with her.\n",
      "and so we go outside and sit amongst the cigarette butts and smoke and i say, wow, that's your little boy.\n",
      "and she says, yeah, isn't he beautiful?\n",
      "and i say, yeah, he is, he is beautiful, he's my light, he keeps me going.\n",
      "she says we finish our cigarettes.\n",
      "she finishes her beer- i don't have a beer cause i can't go home with beer on my breath- and she goes inside to get the keys- she takes too long in there getting the keys and i think something must be wrong.\n",
      "and she comes out and she says: look, i'm really sorry, but, um, like we don't have any gas in the car, it's already on e and he needs to get to work in the morning.\n",
      "and, um, i, you know, i i'm gonna be walk to work as it is.\n",
      "so what i did was, though, here, look, i drew out this map for you and you're really, you're like a mile and a half from home.\n",
      "and um, if you walk three streets over, you'll be back on that pretty street and you just take that and you'll be fine.\n",
      "and she also has, wrapped up in toilet paper, seven cigarettes for me- a third of her pack, i note- and a new pack of matches and she tells me good bye, and that was great to meet you and how lucky and that was fun, and you know let's be friends.\n",
      "and i say yeah, ok, and i walk away but i kind of know we're not gonna be friends.\n",
      "i might not ever see her again and i kind of know i don't think she's ever going to be a vet.\n",
      "and i cross and i walk away and maybe this would've seemed like a visit from my possible future and scary, but it kind of does the opposite.\n",
      "on the walk home i'm like man, that was really grim over there and i'm going home now to my nice boyfriend and he is gonna be so extra happy to see me and we have a one bedroom apartment and we have two trees and there's a yard and we have this jar in the kitchen where there's like loose money that we can use for anything like we would never ever run out of gas and, um, i don't have a baby, you know, so i can leave whenever i want.\n",
      "i smoked all seven cigarettes on the way home and people who have never smoked cigarettes just think ick, disgusting and poison.\n",
      "but unless you've had them and held them dear, you don't know how great they can be and what friends and comfort and kinship they can bring.\n",
      "it took me a long time to quit that boyfriend and then to quit smoking and, uh, sometimes i still miss the smoking.\n"
     ]
    }
   ],
   "source": [
    "for idx,sentence in enumerate(story_sentences):\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "Should we split on phrases or sentences?\n",
    "- The sentences are not exact as the content is spoken are sentence boundaries are imperfectly generated\n",
    "- Phrases are shorter which would allow for more sensible generation to to less-strict/shorter constraints and would be more time efficient \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what, uh, what are they doing out on this suburban street?',\n",
       " \"and the person comes closer and i could see it's a woman.\",\n",
       " 'and then i can see she has her hands in her face.',\n",
       " \"oh, she's crying.\",\n",
       " 'and then she sees me and she composes herself and she gets closer and i see she has no shoes on.',\n",
       " \"she has no shoes on and she's crying and she's out on the street.\"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_sentences[16:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: token [.] not found in list\n",
      "Error: token [ '] not found in list\n",
      "Error: token [.] not found in list\n",
      "Error: token [.] not found in list\n"
     ]
    }
   ],
   "source": [
    "sentences = story_sentences[16:22]\n",
    "alt_sentences = create_syntactic_similars(sentences,num_alternatives=2, num_beams=40, max_bank_size=400,word_bank=word_bank,tokenizer=tokenizer,nlp=nlp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence: \n",
      "what, uh, what are they doing out on this suburban street?\n",
      "alternative sentences: \n",
      " what, yes, what are they doing out on this good day?\n",
      " what, o, what are they doing out on this dark path?\n",
      "input sentence: \n",
      "and the person comes closer and i could see it's a woman.\n",
      "alternative sentences: \n",
      " and the problem goes deeper and i could see it'a thing..\n",
      " and the law requires better and i could see it'a problem..\n",
      "input sentence: \n",
      "and then i can see she has her hands in her face.\n",
      "alternative sentences: \n",
      " and then i can see she has her guns in her hand.\n",
      " and then i can see she has her wounds in her belly.\n",
      "input sentence: \n",
      "oh, she's crying.\n",
      "alternative sentences: \n",
      " o, she'writing. '\n",
      " okay, she'dreaming. '\n",
      "input sentence: \n",
      "and then she sees me and she composes herself and she gets closer and i see she has no shoes on.\n",
      "alternative sentences: \n",
      " and then she turns me and she turns herself and she turns older and i see she has no children on.\n",
      " and then she follows me and she says herself and she says better and i see she has no friends on.\n",
      "input sentence: \n",
      "she has no shoes on and she's crying and she's out on the street.\n",
      "alternative sentences: \n",
      " she has no feet on and she'walking and she'out on the water...\n",
      " she has no lines on and she'playing and she'out on the field...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx,sent in enumerate(sentences):\n",
    "    print(\"input sentence: \")\n",
    "    print(sent)\n",
    "    print(\"alternative sentences: \")\n",
    "\n",
    "    for alt in alt_sentences[idx]:\n",
    "        print(alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n"
     ]
    }
   ],
   "source": [
    "encoder_input_str = \" \"\n",
    "\n",
    "input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams= 200,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1, # dont repeat any words\n",
    "    remove_invalid_values=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=30\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i in range(len(outputs)):\n",
    "    print(tokenizer.decode(outputs[i], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
