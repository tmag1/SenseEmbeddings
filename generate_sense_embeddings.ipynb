{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the textgrids\n",
    "\n",
    "# Rstories are the names of the training (or Regression) stories, which we will use to fit our models\n",
    "Rstories = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy', \n",
    "            'life', 'myfirstdaywiththeyankees', 'naked', \n",
    "            'odetostepfather', 'souls', 'undertheinfluence']\n",
    "\n",
    "# Pstories are the test (or Prediction) stories (well, story), which we will use to test our models\n",
    "Pstories = ['wheretheressmoke']\n",
    "\n",
    "allstories = Rstories + Pstories\n",
    "\n",
    "# Load TextGrids\n",
    "from stimulus_utils import load_grids_for_stories\n",
    "grids = load_grids_for_stories(allstories)\n",
    "\n",
    "# Load TRfiles\n",
    "from stimulus_utils import load_generic_trfiles\n",
    "trfiles = load_generic_trfiles(allstories)\n",
    "\n",
    "# Make word and phoneme datasequences\n",
    "from dsutils import make_word_ds, make_phoneme_ds\n",
    "wordseqs = make_word_ds(grids, trfiles) # dictionary of {storyname : word DataSequence}\n",
    "phonseqs = make_phoneme_ds(grids, trfiles) # dictionary of {storyname : phoneme DataSequence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3218 words in the story called 'naked'\n"
     ]
    }
   ],
   "source": [
    "naked = wordseqs[\"naked\"]\n",
    "\n",
    "# naked.data is a list of all the words in the story\n",
    "print (\"There are %d words in the story called 'naked'\" % len(list(naked.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/thomasmcgall/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Only run these once: This just downloads the punkt tokenizer, which is necessary for the word_tokenize function and the list of stopwords\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from LMMS.transformers_encoder import TransformersEncoder\n",
    "from LMMS.vectorspace import SensesVSM\n",
    "\n",
    "import spacy\n",
    "en_nlp = spacy.load('en_core_web_sm')  # required for lemmatization and POS-tagging\n",
    "\n",
    "from LMMS.wn_utils import WN_Utils\n",
    "wn_utils = WN_Utils()  # WordNet auxilliary methods (just for describing results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLM and sense embeddings ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# NLM/LMMS paths and parameters\n",
    "vecs_path = '/Users/thomasmcgall/Desktop/research/LMMS/data/vectors/lmms-sp-wsd.albert-xxlarge-v2.vectors.txt'\n",
    "wsd_encoder_cfg = {\n",
    "    'model_name_or_path': 'albert-xxlarge-v2',\n",
    "    'min_seq_len': 0,\n",
    "    'max_seq_len': 512,\n",
    "    'layers': [-n for n in range(1, 12 + 1)],  # all layers, with reversed indices\n",
    "    'layer_op': 'ws',\n",
    "    'weights_path': '/Users/thomasmcgall/Desktop/research/LMMS/data/weights/lmms-sp-wsd.albert-xxlarge-v2.weights.txt'\n",
    ",\n",
    "    'subword_op': 'mean'\n",
    "}\n",
    "\n",
    "print('Loading NLM and sense embeddings ...')  # (takes a while)\n",
    "wsd_encoder = TransformersEncoder(wsd_encoder_cfg)\n",
    "senses_vsm = SensesVSM(vecs_path, normalize=True)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = ' '.join(naked.data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPunctuationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy name is Clara and I live in Berkeley California Ist das eine Frage Frau MÃ¼ller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrestore_punctuation(text)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/deepmultilingualpunctuation/punctuationmodel.py:11\u001b[0m, in \u001b[0;36mPunctuationModel.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m,model, grouped_entities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouped_entities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/__init__.py:905\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 905\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    916\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/pipelines/base.py:279\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    281\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:3869\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3860\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   3861\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3862\u001b[0m         state_dict,\n\u001b[1;32m   3863\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3867\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   3868\u001b[0m     )\n\u001b[0;32m-> 3869\u001b[0m     error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3870\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3871\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3872\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   3873\u001b[0m \n\u001b[1;32m   3874\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:626\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 626\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:624\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:624\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 624 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:624\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:620\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    618\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 620\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large\n",
    "\n",
    "model = PunctuationModel()\n",
    "text = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau MÃ¼ller\"\n",
    "result = model.restore_punctuation(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also just ask ChatGpt to punctuate but need paid API key\n",
    "import requests\n",
    "\n",
    "API_KEY = '<API_KEY>'\n",
    "\n",
    "# The OpenAI API URL for ChatGPT\n",
    "API_URL = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {API_KEY}',\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "\n",
    "data = {\n",
    "    'model': 'gpt-3.5-turbo',  \n",
    "    'messages': [\n",
    "        {'role': 'user', 'content': f\"Punctuate the following text: {input_text}\"}\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    if response_data['choices']:\n",
    "        answer = response_data['choices'][0]['message']['content']\n",
    "        print(\"Answer from ChatGPT:\", answer.strip())\n",
    "    else:\n",
    "        print(\"No answer received from ChatGPT.\")\n",
    "else:\n",
    "    print(\"Failed to get a response from the API. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "#get the surrounding words of a given index\n",
    "def get_surrounding_words(index, context_size, words):\n",
    "    # If context size is larger than the number of words, set context size to the number of words\n",
    "    if context_size > len(words):\n",
    "        context_size = len(words)\n",
    "\n",
    "    half_window = context_size // 2\n",
    "    word_count = len(words)\n",
    "    \n",
    "    start_index = index - half_window\n",
    "    end_index = index + half_window   + context_size % 2  \n",
    "    \n",
    "    if start_index < 0:\n",
    "        surrounding_words = words[0:context_size] \n",
    "        relative_word_idx = index\n",
    "    elif end_index > word_count:\n",
    "        surrounding_words = words[word_count - context_size:word_count]\n",
    "        relative_word_idx = index - (word_count - context_size)\n",
    "\n",
    "    else:\n",
    "        surrounding_words = words[start_index:end_index]\n",
    "        relative_word_idx = context_size//2 \n",
    "\n",
    "    return ' '.join(surrounding_words), relative_word_idx\n",
    "\n",
    "# Ensure that this model is not adding/removing any words\n",
    "def check_word_count(input_text,output_text):\n",
    "    # Tokenize the string into words and punctuation\n",
    "    tokens_input = wordpunct_tokenize(input_text)\n",
    "    # Remove tokens that are entirely punctuation, mixed(punctuation and word) tokens may include datees, emails, contractions, ...\n",
    "    words = [token for token in tokens_input if not all(char in punctuation for char in token)]\n",
    "    # get number of word tokens\n",
    "    input_num= len(words)\n",
    "\n",
    "    tokens_output = wordpunct_tokenize(output_text)\n",
    "\n",
    "    words = [token for token in tokens_output if not all(char in punctuation for char in token)]\n",
    "    output_num= len(words)\n",
    "\n",
    "    if(input_num != output_num):\n",
    "        warnings.warn(\"Model may have introduced or removed words. Please check the model.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sense_embeddings(input_text, context_length = 'sentence'):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_text : string\n",
    "        The text for which to generate sense embeddings.\n",
    "    context_length : int or string('sentence')\n",
    "        The number of words to use as context for each word in the input text. \n",
    "        If 'sentence', the entire sentence will be used as context for each word.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sense_embeddings, missing_sense_embeddings, sentences\n",
    "\n",
    "    sense_embeddings: list of tuples\n",
    "        tuple - (word, sense, sense_embedding)\n",
    "            word: str:\n",
    "                The word in the input text for which embeddings were generated.\n",
    "            sense: str:\n",
    "                The sense of the word in the input text.\n",
    "                ex)'grow%2:30:04::'\n",
    "            sense_embedding: np.array\n",
    "                The sense embedding of the word in the input text.\n",
    "    missing_sense_embeddings: dict\n",
    "        A dictionary of non-stopwords that do not have sense embeddings\n",
    "        dictionary - {word: count of apperances in input text}\n",
    "    sentences - list\n",
    "        list of sentences in the input text. \n",
    "\n",
    "\n",
    "    '''\n",
    "    #list of stopwords from nltk for reference\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #dictionary that maps from word(missing sense embedding) to the number of times it appears in the input text\n",
    "    missing_sense_embeddings = defaultdict(int)\n",
    "\n",
    "    # context length can be 'sentence' or int(words) where words is the number of words to use as context \n",
    "    if context_length == 'sentence':\n",
    "        # May need to split text into chunks before feeding the punctuator?\n",
    "\n",
    "        # Punctuate Sentences\n",
    "        punctuation_model = PunctuationModel()\n",
    "        punctuated_output = punctuation_model.restore_punctuation(input_text)\n",
    "\n",
    "        # Will raise warning if number of non-punctuation tokens in input text is not equal to output text\n",
    "        # NOTE: This is not a perfect check as new tokens may be generated that are a mix of punctuation and existing words\n",
    "        check_word_count(input_text,punctuated_output)\n",
    "\n",
    "        # Split sentences by Punctuation\n",
    "        sentences = nltk.sent_tokenize(punctuated_output)  \n",
    "\n",
    "        sense_embeddings = []\n",
    "\n",
    "        for idx,sentence in enumerate(sentences) :\n",
    "            print(f\"{idx+1}/{len(sentences)}\")\n",
    "            for word_idx in range(0,len(sentence.split())):\n",
    "                context = sentence\n",
    "                target_idxs = [word_idx]  # for 'mouse'\n",
    "\n",
    "                # use spacy to automatically determine lemma and POS (replace with your favorite NLP toolkit)\n",
    "                doc = en_nlp(context)\n",
    "                target_lemma = '_'.join([doc[i].lemma_ for i in target_idxs])\n",
    "                target_pos = doc[target_idxs[0]].pos_\n",
    "\n",
    "                # retrieve contextual embedding for target token/span\n",
    "                tokens = [t.text for t in doc]\n",
    "                ctx_embeddings = wsd_encoder.token_embeddings([tokens])[0]\n",
    "                target_embedding = np.array([ctx_embeddings[i][1] for i in target_idxs]).mean(axis=0)\n",
    "                target_embedding = target_embedding / np.linalg.norm(target_embedding)\n",
    "\n",
    "\n",
    "                # find sense embeddings that are nearest-neighbors to the target contextual embedding\n",
    "                # candidates restricted by lemma and part-of-speech\n",
    "                matches = senses_vsm.match_senses(target_embedding, lemma=target_lemma, postag=target_pos, topn=3)\n",
    "\n",
    "                word = context.split()[word_idx]\n",
    "                if len(matches) == 0:\n",
    "                    # No sense embeddings found, append (,)\n",
    "                    sense_embeddings.append((word,None,None))\n",
    "                    #increment count for missing embedding\n",
    "                    if word not in stop_words:\n",
    "                        missing_sense_embeddings[word] += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    senses_vsm.get_vec(matches[0][0])\n",
    "                    sense_embeddings.append((word,matches[0][0],senses_vsm.get_vec(matches[0][0])))\n",
    "\n",
    "        return sense_embeddings, missing_sense_embeddings,sentences\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert type(context_length) is int, \"context_length must be an integer or 'sentence'\"\n",
    "        sense_embeddings = []\n",
    "\n",
    "        number_of_words_input = len(input_text.split())\n",
    "        for word_idx in tqdm(range(0, number_of_words_input), desc=\"Processing\"):   \n",
    "            # assuming context is centered around the word of interest\n",
    "            context,relative_idx = get_surrounding_words(word_idx, context_length, input_text.split())\n",
    "\n",
    "            # print(context, relative_idx)\n",
    "            \n",
    "            target_idxs = [relative_idx] \n",
    "\n",
    "            # use spacy to automatically determine lemma and POS (replace with your favorite NLP toolkit)\n",
    "            doc = en_nlp(context)\n",
    "            target_lemma = '_'.join([doc[i].lemma_ for i in target_idxs])\n",
    "            target_pos = doc[target_idxs[0]].pos_\n",
    "\n",
    "\n",
    "            # retrieve contextual embedding for target token/span\n",
    "            tokens = [t.text for t in doc]\n",
    "            ctx_embeddings = wsd_encoder.token_embeddings([tokens])[0]\n",
    "            target_embedding = np.array([ctx_embeddings[i][1] for i in target_idxs]).mean(axis=0)\n",
    "            target_embedding = target_embedding / np.linalg.norm(target_embedding)\n",
    "\n",
    "\n",
    "            # find sense embeddings that are nearest-neighbors to the target contextual embedding\n",
    "            # candidates restricted by lemma and part-of-speech\n",
    "            matches = senses_vsm.match_senses(target_embedding, lemma=target_lemma, postag=target_pos, topn=3)\n",
    "\n",
    "            # print(len(matches))\n",
    "            word = context.split()[relative_idx]\n",
    "            if len(matches) == 0:\n",
    "                # No sense embeddings found, append (,)\n",
    "                sense_embeddings.append((word,None,None))\n",
    "                #increment count for missing embedding\n",
    "                if word not in stop_words:\n",
    "                    missing_sense_embeddings[word] += 1\n",
    "                continue\n",
    "            else:\n",
    "                senses_vsm.get_vec(matches[0][0])\n",
    "                sense_embeddings.append((word,matches[0][0],senses_vsm.get_vec(matches[0][0])))\n",
    "\n",
    "        return sense_embeddings, missing_sense_embeddings,sentences\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage \n",
    "life = wordseqs[\"life\"]\n",
    "len(life.data)\n",
    "\n",
    "#get first 20 words of the story\n",
    "input_text = ' '.join(life.data)\n",
    "\n",
    "embeddings,missing_embeddings,sentences = generate_sense_embeddings(input_text, context_length = 'sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so : so%4:02:07::, (4096,)\n",
      "i : No sense, No embedding\n",
      "get : get%2:30:03::, (4096,)\n",
      "a : No sense, No embedding\n",
      "phone : phone%1:06:00::, (4096,)\n",
      "call : call%1:10:01::, (4096,)\n",
      "from : No sense, No embedding\n",
      "my : No sense, No embedding\n",
      "mom : mom%1:18:00::, (4096,)\n",
      "and : No sense, No embedding\n"
     ]
    }
   ],
   "source": [
    "for word, sense, embedding in embeddings[:10]:\n",
    "    embedding_info = embedding.shape if embedding is not None and embedding.size > 0 else \"No embedding\"\n",
    "    sense_info = sense if sense else \"No sense\"\n",
    "    print(f\"{word} : {sense_info}, {embedding_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['montana', 'kinda', 'telling', 'worry,', 'gonna', 'ok,', \"we're\", 'this,', 'alright.', 'something']\n"
     ]
    }
   ],
   "source": [
    "print(list(missing_embeddings.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so i get a phone call from my mom and she tells me that my father is about to get on an emergency life flight from our home in montana to go to denver to get an emergency liver transplant.\n",
      "my mom is kinda perennially optimistic and she's telling me: don't worry, it's gonna be ok, we're gonna pull through this, it's gonna be alright.\n",
      "but i know something is is really wrong.\n",
      "so i get the next flight i can to go from where i'm living here in new york, hoping that i get there before my father dies, and i'm really glad i i got that flight as fast as i did because i was able to spend a couple hours with my father before he passed away.\n",
      "and before i know it i'm at this side of his hospital bed with my mom and we're sobbing because he's passed.\n",
      "um, my dad was a. he was a strong, silent type.\n",
      "he was um, he was a grew up on a farm and he was the town one of one of two town eye doctors.\n",
      "so he could fix anything.\n",
      "you know, he could fix tractors or eyes.\n",
      "he could, you know, no matter what, and he was always doing it like behind the scenes.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences[:10]:\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
