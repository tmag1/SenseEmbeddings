{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the textgrids\n",
    "\n",
    "# Rstories are the names of the training (or Regression) stories, which we will use to fit our models\n",
    "Rstories = ['alternateithicatom', 'avatar', 'howtodraw', 'legacy', \n",
    "            'life', 'myfirstdaywiththeyankees', 'naked', \n",
    "            'odetostepfather', 'souls', 'undertheinfluence']\n",
    "\n",
    "# Pstories are the test (or Prediction) stories (well, story), which we will use to test our models\n",
    "Pstories = ['wheretheressmoke']\n",
    "\n",
    "allstories = Rstories + Pstories\n",
    "\n",
    "# Load TextGrids\n",
    "from stimulus_utils import load_grids_for_stories\n",
    "grids = load_grids_for_stories(allstories)\n",
    "\n",
    "# Load TRfiles\n",
    "from stimulus_utils import load_generic_trfiles\n",
    "trfiles = load_generic_trfiles(allstories)\n",
    "\n",
    "# Make word and phoneme datasequences\n",
    "from dsutils import make_word_ds, make_phoneme_ds\n",
    "wordseqs = make_word_ds(grids, trfiles) # dictionary of {storyname : word DataSequence}\n",
    "phonseqs = make_phoneme_ds(grids, trfiles) # dictionary of {storyname : phoneme DataSequence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3218 words in the story called 'naked'\n"
     ]
    }
   ],
   "source": [
    "naked = wordseqs[\"naked\"]\n",
    "\n",
    "# naked.data is a list of all the words in the story\n",
    "print (\"There are %d words in the story called 'naked'\" % len(list(naked.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "\n",
    "# Only run once: This just downloads the punkt tokenizer, which is necessary for the word_tokenize function \n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from LMMS.transformers_encoder import TransformersEncoder\n",
    "from LMMS.vectorspace import SensesVSM\n",
    "\n",
    "import spacy\n",
    "en_nlp = spacy.load('en_core_web_sm')  # required for lemmatization and POS-tagging\n",
    "\n",
    "from LMMS.wn_utils import WN_Utils\n",
    "wn_utils = WN_Utils()  # WordNet auxilliary methods (just for describing results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLM and sense embeddings ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# NLM/LMMS paths and parameters\n",
    "vecs_path = '/Users/thomasmcgall/Desktop/research/LMMS/data/vectors/lmms-sp-wsd.albert-xxlarge-v2.vectors.txt'\n",
    "wsd_encoder_cfg = {\n",
    "    'model_name_or_path': 'albert-xxlarge-v2',\n",
    "    'min_seq_len': 0,\n",
    "    'max_seq_len': 512,\n",
    "    'layers': [-n for n in range(1, 12 + 1)],  # all layers, with reversed indices\n",
    "    'layer_op': 'ws',\n",
    "    'weights_path': '/Users/thomasmcgall/Desktop/research/LMMS/data/weights/lmms-sp-wsd.albert-xxlarge-v2.weights.txt'\n",
    ",\n",
    "    'subword_op': 'mean'\n",
    "}\n",
    "\n",
    "print('Loading NLM and sense embeddings ...')  # (takes a while)\n",
    "wsd_encoder = TransformersEncoder(wsd_encoder_cfg)\n",
    "senses_vsm = SensesVSM(vecs_path, normalize=True)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at SJ-Ray/Re-Punctuate.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Hugging face model for inserting punctuation\n",
    "\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration, T5ForConditionalGeneration\n",
    "\n",
    "punctuation_tokenizer = T5Tokenizer.from_pretrained('SJ-Ray/Re-Punctuate')\n",
    "punctuation_model = TFT5ForConditionalGeneration.from_pretrained('SJ-Ray/Re-Punctuate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get the surrounding words of a given index\n",
    "def get_surrounding_words(index, context_size, words):\n",
    "    # If context size is larger than the number of words, set context size to the number of words\n",
    "    if context_size > len(words):\n",
    "        context_size = len(words)\n",
    "\n",
    "    half_window = context_size // 2\n",
    "    word_count = len(words)\n",
    "    \n",
    "    start_index = index - half_window\n",
    "    end_index = index + half_window   + context_size % 2  \n",
    "    \n",
    "    if start_index < 0:\n",
    "        surrounding_words = words[0:context_size] \n",
    "        relative_word_idx = index\n",
    "    elif end_index > word_count:\n",
    "        surrounding_words = words[word_count - context_size:word_count]\n",
    "        relative_word_idx = index - (word_count - context_size)\n",
    "\n",
    "    else:\n",
    "        surrounding_words = words[start_index:end_index]\n",
    "        relative_word_idx = context_size//2 \n",
    "\n",
    "    return ' '.join(surrounding_words), relative_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sense_embeddings(input_text, context_length = 'sentence'):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_text : string\n",
    "        The text for which to generate sense embeddings.\n",
    "    context_length : int or string('sentence')\n",
    "        The number of words to use as context for each word in the input text. \n",
    "        If 'sentence', the entire sentence will be used as context for each word.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    word, sense, sense_embedding : tuple\n",
    "\n",
    "    word: str:\n",
    "        The word in the input text for which embeddings were generated.\n",
    "    sense: str:\n",
    "        The sense of the word in the input text.\n",
    "        ex)'grow%2:30:04::'\n",
    "    sense_embedding: np.array\n",
    "        The sense embedding of the word in the input text.\n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "    # context length can be 'sentence' or int(words) where words is the number of words to use as context \n",
    "    if context_length == 'sentence':\n",
    "        # Punctuate Sentences\n",
    "        inputs = punctuation_tokenizer.encode(\"punctuate: \" + input_text, return_tensors=\"tf\") \n",
    "        result = punctuation_model.generate(inputs)\n",
    "        punctuated_output =  punctuation_tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "\n",
    "        # Split sentences by Punctuation\n",
    "        sentences = nltk.sent_tokenize(punctuated_output)  \n",
    "\n",
    "        sense_embeddings = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            print(sentence)\n",
    "            for word_idx in range(0,len(sentence.split())):\n",
    "                context = sentence\n",
    "                target_idxs = [word_idx]  # for 'mouse'\n",
    "\n",
    "                # use spacy to automatically determine lemma and POS (replace with your favorite NLP toolkit)\n",
    "                doc = en_nlp(context)\n",
    "                target_lemma = '_'.join([doc[i].lemma_ for i in target_idxs])\n",
    "                target_pos = doc[target_idxs[0]].pos_\n",
    "\n",
    "                # retrieve contextual embedding for target token/span\n",
    "                tokens = [t.text for t in doc]\n",
    "                ctx_embeddings = wsd_encoder.token_embeddings([tokens])[0]\n",
    "                target_embedding = np.array([ctx_embeddings[i][1] for i in target_idxs]).mean(axis=0)\n",
    "                target_embedding = target_embedding / np.linalg.norm(target_embedding)\n",
    "\n",
    "\n",
    "                # find sense embeddings that are nearest-neighbors to the target contextual embedding\n",
    "                # candidates restricted by lemma and part-of-speech\n",
    "                matches = senses_vsm.match_senses(target_embedding, lemma=target_lemma, postag=target_pos, topn=3)\n",
    "\n",
    "                if len(matches) == 0:\n",
    "                    # No sense embeddings found, append (,)\n",
    "                    sense_embeddings.append((context.split()[word_idx],None,None))\n",
    "                    continue\n",
    "                else:\n",
    "                    senses_vsm.get_vec(matches[0][0])\n",
    "                    sense_embeddings.append((context.split()[word_idx],matches[0][0],senses_vsm.get_vec(matches[0][0])))\n",
    "\n",
    "        return sense_embeddings\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert type(context_length) is int, \"context_length must be an integer or 'sentence'\"\n",
    "        sense_embeddings = []\n",
    "\n",
    "        number_of_words_input = len(input_text.split())\n",
    "        for word_idx in range(0,number_of_words_input):\n",
    "            # assuming context is centered around the word of interest\n",
    "            context,relative_idx = get_surrounding_words(word_idx, context_length, input_text.split())\n",
    "\n",
    "            # print(context, relative_idx)\n",
    "            \n",
    "            target_idxs = [relative_idx] \n",
    "\n",
    "            # use spacy to automatically determine lemma and POS (replace with your favorite NLP toolkit)\n",
    "            doc = en_nlp(context)\n",
    "            target_lemma = '_'.join([doc[i].lemma_ for i in target_idxs])\n",
    "            target_pos = doc[target_idxs[0]].pos_\n",
    "\n",
    "\n",
    "            # retrieve contextual embedding for target token/span\n",
    "            tokens = [t.text for t in doc]\n",
    "            ctx_embeddings = wsd_encoder.token_embeddings([tokens])[0]\n",
    "            target_embedding = np.array([ctx_embeddings[i][1] for i in target_idxs]).mean(axis=0)\n",
    "            target_embedding = target_embedding / np.linalg.norm(target_embedding)\n",
    "\n",
    "\n",
    "            # find sense embeddings that are nearest-neighbors to the target contextual embedding\n",
    "            # candidates restricted by lemma and part-of-speech\n",
    "            matches = senses_vsm.match_senses(target_embedding, lemma=target_lemma, postag=target_pos, topn=3)\n",
    "\n",
    "            # print(len(matches))\n",
    "            if len(matches) == 0:\n",
    "                # No sense embeddings found, append (,)\n",
    "                sense_embeddings.append((context.split()[relative_idx],None,None))\n",
    "                continue\n",
    "            else:\n",
    "                senses_vsm.get_vec(matches[0][0])\n",
    "                sense_embeddings.append((context.split()[relative_idx],matches[0][0],senses_vsm.get_vec(matches[0][0])))\n",
    "\n",
    "        return sense_embeddings\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so i get a phone call from my mom and she tells me that my father is about to get\n"
     ]
    }
   ],
   "source": [
    "#example usage \n",
    "life = wordseqs[\"life\"]\n",
    "#get first 20 words of the story\n",
    "firstWords = ' '.join(life.data[:20])\n",
    "print(firstWords)\n",
    "\n",
    "embeddings = generate_sense_embeddings(firstWords, context_length = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so : so%4:02:09::, (4096,)\n",
      "i : No sense, No embedding\n",
      "get : get%2:39:00::, (4096,)\n",
      "a : No sense, No embedding\n",
      "phone : phone%1:06:00::, (4096,)\n",
      "call : call%1:10:01::, (4096,)\n",
      "from : No sense, No embedding\n",
      "my : No sense, No embedding\n",
      "mom : mom%1:18:00::, (4096,)\n",
      "and : No sense, No embedding\n",
      "she : No sense, No embedding\n",
      "tells : tell%2:32:04::, (4096,)\n",
      "me : No sense, No embedding\n",
      "that : No sense, No embedding\n",
      "my : No sense, No embedding\n",
      "father : father%1:18:00::, (4096,)\n",
      "is : No sense, No embedding\n",
      "about : about%5:00:00:active:01, (4096,)\n",
      "to : No sense, No embedding\n",
      "get : get%2:33:00::, (4096,)\n"
     ]
    }
   ],
   "source": [
    "for word, sense, embedding in embeddings:\n",
    "    embedding_info = embedding.shape if embedding is not None and embedding.size > 0 else \"No embedding\"\n",
    "    sense_info = sense if sense else \"No sense\"\n",
    "    print(f\"{word} : {sense_info}, {embedding_info}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
